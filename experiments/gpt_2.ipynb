{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ddmplzirfqo2"
      },
      "outputs": [],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from datasets import load_dataset\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "tTN85cS5fuEH"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"imdb\")\n",
        "inputs = dataset['train']['text'][:5]"
      ],
      "metadata": {
        "id": "TJkoRctrpM8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"glue\", \"mrpc\")\n",
        "inputs = dataset['train']['sentence1'][:5]"
      ],
      "metadata": {
        "id": "Dzg_7fzZpOo5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"uer/gpt2-chinese-cluecorpussmall\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"uer/gpt2-chinese-cluecorpussmall\")\n",
        "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "encoded_input = tokenizer(inputs, padding = 'longest', return_tensors='pt', truncation=True)"
      ],
      "metadata": {
        "id": "XTamNV2OoW2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelWithLMHead\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Graphcore/gpt2-wikitext-103\")\n",
        "model = AutoModelWithLMHead.from_pretrained(\"Graphcore/gpt2-wikitext-103\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "encoded_input = tokenizer(inputs, padding = 'longest', return_tensors='pt', truncation=True)"
      ],
      "metadata": {
        "id": "0MwE8NNGpUTJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from datasets import load_dataset\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "encoded_input = tokenizer(inputs, padding = 'longest', return_tensors='pt', truncation=True)"
      ],
      "metadata": {
        "id": "Po3GZOe4fvrm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class GPT2FeatureExtractor(nn.Module):\n",
        "    def __init__(self, gpt2_model):\n",
        "        super(GPT2FeatureExtractor, self).__init__()\n",
        "        self.gpt2_model = gpt2_model\n",
        "        self.layer_features = {'ln_1': [], 'ln_2': [], 'attn': [], 'mlp': []}\n",
        "        self.nlf_features = []  # Updated to store as a list\n",
        "        self.embedding_features = None  # Updated to store as a tensor\n",
        "\n",
        "        # Register hooks for each module\n",
        "        for i, layer in enumerate(self.gpt2_model.transformer.h):\n",
        "            layer_name = f\"layer_{i + 1}\"\n",
        "            self.layer_features[layer_name] = {\n",
        "                'ln_1': None,\n",
        "                'ln_2': None,\n",
        "                'attn': None,\n",
        "                'mlp': None\n",
        "            }\n",
        "\n",
        "            layer.ln_1.register_forward_hook(self.ln_1_hook(layer_name))\n",
        "            layer.ln_2.register_forward_hook(self.ln_2_hook(layer_name))\n",
        "            layer.attn.register_forward_hook(self.attn_hook(layer_name))\n",
        "            layer.mlp.register_forward_hook(self.mlp_hook(layer_name))\n",
        "\n",
        "        # Register hook for ln_f after the last layer\n",
        "        self.gpt2_model.transformer.ln_f.register_forward_hook(self.nlf_hook)\n",
        "\n",
        "        # Register hook for the embedding layer\n",
        "        self.gpt2_model.transformer.wte.register_forward_hook(self.embedding_hook)\n",
        "\n",
        "    def ln_1_hook(self, layer_name):\n",
        "        def hook(module, input, output):\n",
        "            self.layer_features[layer_name]['ln_1'] = output\n",
        "        return hook\n",
        "\n",
        "    def ln_2_hook(self, layer_name):\n",
        "        def hook(module, input, output):\n",
        "            self.layer_features[layer_name]['ln_2'] = output\n",
        "        return hook\n",
        "\n",
        "    def attn_hook(self, layer_name):\n",
        "        def hook(module, input, output):\n",
        "            self.layer_features[layer_name]['attn'] = output\n",
        "        return hook\n",
        "\n",
        "    def mlp_hook(self, layer_name):\n",
        "        def hook(module, input, output):\n",
        "            self.layer_features[layer_name]['mlp'] = output\n",
        "        return hook\n",
        "\n",
        "    def nlf_hook(self, module, input, output):\n",
        "        # Store the hidden features of ln_f after the last layer\n",
        "        self.nlf_features.append(output)\n",
        "\n",
        "    def embedding_hook(self, module, input, output):\n",
        "        # Store the hidden features of the embedding layer\n",
        "        self.embedding_features = output\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        # Reset hidden features\n",
        "        for layer_name in self.layer_features:\n",
        "            for sub_module in self.layer_features[layer_name]:\n",
        "                self.layer_features[layer_name][sub_module] = None\n",
        "\n",
        "        self.nlf_features = []  # Reset ln_f features\n",
        "        self.embedding_features = None  # Reset embedding features\n",
        "\n",
        "        # Forward pass through the GPT-2 model\n",
        "        outputs = self.gpt2_model(input_ids, attention_mask=attention_mask)\n",
        "\n",
        "        # Return the stored hidden features\n",
        "        return {\n",
        "            'layers': self.layer_features,\n",
        "            'nlf': self.nlf_features,\n",
        "            'embedding': self.embedding_features\n",
        "        }"
      ],
      "metadata": {
        "id": "90MPcuMug1A2"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_extractor = GPT2FeatureExtractor(model)\n",
        "hidden_features = feature_extractor(encoded_input['input_ids'],encoded_input['attention_mask'])"
      ],
      "metadata": {
        "id": "fQCxf1Bkg4F2"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_features['embedding'].shape"
      ],
      "metadata": {
        "id": "QC-QIdxj0YNr",
        "outputId": "7702c838-61a4-4144-a59f-4ef743f131e2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([5, 455, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_input['input_ids'].shape"
      ],
      "metadata": {
        "id": "_5CssYAp1jg3",
        "outputId": "174bf259-7d04-49c0-eeac-9a6a8a35cfad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([5, 455])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "main = torch.empty(0)\n",
        "for i in range (encoded_input['input_ids'].shape[0]):\n",
        "  temp = torch.empty(0)\n",
        "  shape = hidden_features['embedding'][0, :, :].shape[0]\n",
        "  temp = torch.cat((temp, hidden_features['embedding'][i, :, :].reshape(768, shape)), 0)\n",
        "  for j in range(1,13):\n",
        "    temp = torch.cat((temp, hidden_features['layers'][f'layer_{j}']['ln_1'][i,:,:].reshape(768, shape)),0)\n",
        "    temp = torch.cat((temp, hidden_features['layers'][f'layer_{j}']['attn'][0][i,:,:].reshape(768, shape)), 0)\n",
        "    temp = torch.cat((temp, hidden_features['layers'][f'layer_{j}']['attn'][1][0][i,:,:].reshape(64*12, shape)), 0)\n",
        "    temp = torch.cat((temp, hidden_features['layers'][f'layer_{j}']['attn'][1][1][i,:,:].reshape(64*12, shape)), 0)\n",
        "    temp = torch.cat((temp, hidden_features['layers'][f'layer_{j}']['ln_2'][i,:,:].reshape(768, shape)),0)\n",
        "    temp = torch.cat((temp, hidden_features['layers'][f'layer_{j}']['mlp'][i,:,:].reshape(768, shape)),0)\n",
        "  main = torch.cat((main, temp), 1)"
      ],
      "metadata": {
        "id": "p2QIdMhRioD8"
      },
      "execution_count": 29,
      "outputs": []
    }
  ]
}